{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import sys\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from explorerl.agents import BaseAgent\n",
    "from explorerl.utils import *\n",
    "from explorerl.REINFORCE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([*env.action_space.shape + (2,)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTf(BaseAgent):\n",
    "    def __init__(self,gamma=1.0,learning_rate=0.001, featurizer=None,scaler=None,use_bias = False,model=None):\n",
    "        super(ActorCriticTf, self).__init__(gamma,learning_rate,featurizer,scaler,use_bias,has_replay=True)\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.name = \"ActorCriticTf\"\n",
    "    \n",
    "    def initialize_model(self,observation_space,action_space):\n",
    "        self.observation_space = observation_space[0]\n",
    "        self.action_space = action_space\n",
    "        input_space = self.observation_space  \n",
    "        if self.featurizer:\n",
    "            input_space = self.featurizer.transform([np.ones(self.observation_space)]).flatten().shape[0]\n",
    "        if self.use_bias:\n",
    "            input_space += 1\n",
    "       \n",
    "        model = LinearEstimatorTf(input_space=input_space,output_space=self.action_space,softmax=True)\n",
    "        self.model[\"outputs\"] = model\n",
    "        \n",
    "        def log_loss(model,predictions,targets):\n",
    "            return -1*(tf.reduce_sum(tf.multiply(tf.math.log(predictions),targets))) + tf.add_n(model.losses)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        def train_step(model,inputs,targets):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(inputs)\n",
    "                total_loss = log_loss(model,predictions,targets)\n",
    "            gradients = tape.gradient(total_loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "                \n",
    "        self.model[\"loss\"] = log_loss\n",
    "        self.model[\"training_op\"] = train_step\n",
    "        print(\"Model Created!\")\n",
    "\n",
    "    def train_policy(self):\n",
    "        return self.stochastic()\n",
    "    \n",
    "    def test_policy(self):\n",
    "        return self.stochastic()\n",
    "    \n",
    "    def stochastic(self):\n",
    "        def act(obs):\n",
    "            estimator = self.model[\"outputs\"]\n",
    "            probs = estimator(obs)\n",
    "            return np.random.choice(self.action_space,p=np.array(probs[0])) , probs\n",
    "        return act\n",
    "    \n",
    "    def greedy(self):\n",
    "        def act(obs):\n",
    "            estimator = self.model[\"outputs\"]\n",
    "            probs = estimator(obs)\n",
    "            return np.argmax(probs[0]) , probs\n",
    "        return act\n",
    "    \n",
    "    def episodal_train_iter(self,policy):\n",
    "        #has experience memory, but only updates \n",
    "        obs_arr = []\n",
    "        reward_arr = []\n",
    "        training_op = self.model[\"training_op\"]\n",
    "        for obs, action, next_obs, reward, done in self.experience_replay:\n",
    "            reward_arr.append(reward)\n",
    "        dr = self.discount_reward(reward_arr)\n",
    "        for i in range(len(reward_arr)):\n",
    "            obs, action, next_obs, reward, done = self.experience_replay[i]\n",
    "            target = np.zeros((1,self.action_space))\n",
    "            target[0][action] = dr[i]\n",
    "            training_op(self.model[\"outputs\"],obs,target)\n",
    "        self.experience_replay = deque([])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
