{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import sys\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from explorerl.agents import BaseAgent\n",
    "from explorerl.utils import *\n",
    "from explorerl.REINFORCE import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPolicyValueHeadTf(tf.keras.Model):\n",
    "    def __init__(self,input_space,output_space,configs={}):\n",
    "        super(DualPolicyValueHeadTf, self).__init__()\n",
    "        value = []\n",
    "        #value head\n",
    "        value.append(create_linear_tf(input_dims=input_space,output_dims=32,relu=True))\n",
    "        value.append(create_linear_tf(input_dims=(32,),output_dims=1,relu=False))\n",
    "        self.valuehead = tf.keras.models.Sequential(value)\n",
    "        #policy head\n",
    "        policy = []\n",
    "        policy.append(create_linear_tf(input_dims=input_space,output_dims=32,relu=True))\n",
    "        policy.append(create_linear_tf(input_dims=(32,),output_dims=output_space,relu=False))\n",
    "        self.policyhead = tf.keras.models.Sequential(policy)        \n",
    "        if \"softmax\" in configs:\n",
    "            self.policyhead = tf.keras.models.Sequential([self.policyhead,tf.keras.layers.Softmax()])\n",
    "            \n",
    "    def call(self,x,training=True):\n",
    "        value = self.valuehead(x)\n",
    "        policy = self.policyhead(x)\n",
    "        return value, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTf(BaseTfAgent):\n",
    "    def __init__(self,estimator=None,gamma=0.995,learning_rate=0.001, featurizer=None,\n",
    "                scaler=None,configs={\"softmax\"},replay_size=500,replay_batch=32):\n",
    "        super(ActorCriticTf, self).__init__(estimator,gamma,learning_rate,featurizer,scaler,configs,replay_size=replay_size)\n",
    "        self.name = \"ActorCriticTf\"\n",
    "        self.replay_batch = replay_batch\n",
    "    \n",
    "    def initialize_model(self,observation_space,action_space):\n",
    "        super(ActorCriticTf, self).initialize_model(observation_space,action_space)          \n",
    "\n",
    "        def log_loss(model,predictions,targets):\n",
    "            return -1*(tf.reduce_sum(tf.multiply(tf.math.log(predictions),targets)))\n",
    "        \n",
    "        def value_loss_fn(model,predictions,targets):\n",
    "            return tf.reduce_sum(tf.square(tf.subtract(targets,predictions)))\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        def train_step(model,inputs,action,value_target,advantage_target):\n",
    "            with tf.GradientTape() as tape:\n",
    "                value,predictions = model(inputs)\n",
    "                value_loss = value_loss_fn(model.valuehead,value,value_target)\n",
    "                policy_loss = log_loss(model.policyhead,predictions,advantage_target)\n",
    "                loss = value_loss + policy_loss\n",
    "            policy_gradients = tape.gradient(loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(policy_gradients,model.trainable_variables))\n",
    "            del tape\n",
    "                \n",
    "        self.model[\"loss\"] = log_loss\n",
    "        self.model[\"training_op\"] = train_step\n",
    "        print(\"Model Created!\")\n",
    "\n",
    "    def train_policy(self):\n",
    "        return self.stochastic()\n",
    "    \n",
    "    def test_policy(self):\n",
    "        return self.stochastic()\n",
    "    \n",
    "    def stochastic(self):\n",
    "        def act(obs):\n",
    "            estimator = self.model[\"estimator\"]\n",
    "            _, probs = estimator(obs)\n",
    "            if \"continuous\" not in self.configs:\n",
    "                return np.random.choice(self.action_space,p=np.array(probs[0])) , probs\n",
    "        return act\n",
    "    \n",
    "    def greedy(self):\n",
    "        def act(obs):\n",
    "            estimator = self.model[\"estimator\"]\n",
    "            probs = estimator(obs)\n",
    "            return np.argmax(probs[0]) , probs\n",
    "        return act\n",
    "    \n",
    "    def replay(self,policy):\n",
    "        training_op = self.model[\"training_op\"]\n",
    "        for obs, action, next_obs, reward, done in self.experience_replay:\n",
    "            value = self.model[\"estimator\"].valuehead(obs)\n",
    "            next_value = self.model[\"estimator\"].valuehead(next_obs)\n",
    "            target = reward\n",
    "            if done == False:\n",
    "                target += self.gamma*next_value\n",
    "            value_target = tf.stop_gradient(target)\n",
    "            advantage_target = np.zeros((1,self.action_space))\n",
    "            advantage_target[0][action] = tf.stop_gradient(target - value)\n",
    "            training_op(self.model[\"estimator\"],obs,action,value_target,advantage_target)\n",
    "        \n",
    "            \n",
    "    def episodal_train_iter(self,policy):\n",
    "        #has experience memory, but only updates \n",
    "        self.replay(policy)\n",
    "        self.experience_replay = deque(maxlen=self.replay_size)\n",
    "    \n",
    "    def train_iter(self,policy,action,values,obs,next_obs,reward,done):\n",
    "        training_op = self.model[\"training_op\"]\n",
    "        value = self.model[\"estimator\"].valuehead(obs)\n",
    "        next_value = self.model[\"estimator\"].valuehead(next_obs)\n",
    "        target = reward\n",
    "        if done == False:\n",
    "            target += self.gamma*next_value\n",
    "        value_target = tf.stop_gradient(target)\n",
    "        advantage_target = np.zeros((1,self.action_space))\n",
    "        advantage_target[0][action] = tf.stop_gradient(target - value)\n",
    "        training_op(self.model[\"estimator\"],obs,action,value_target,advantage_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "e = EnvRunner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ActorCriticTf(estimator=DualPolicyValueHeadTf,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon : None, Num Steps : 181, Avg Reward with Window Size 100 : 177.78:  92%|█████████▏| 916/1000 [12:51<01:10,  1.19it/s]          "
     ]
    }
   ],
   "source": [
    "stats = e.train(a,episodes=1000,train_episodal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.test(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "e = EnvRunner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander Actor Critic (TD learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "e = EnvRunner(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ActorCriticTf(estimator=DualPolicyValueHeadTf,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = e.train(a,episodes=2500,train_episodal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.test(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPolicyValueHeadTorch(torch.nn.Module):\n",
    "    def __init__(self,input_space,output_space,configs={}):\n",
    "        super(DualPolicyValueHeadTorch, self).__init__()\n",
    "        value = []\n",
    "        #value head\n",
    "        value.append(create_linear_torch(input_dims=input_space[0],output_dims=1,relu=False))\n",
    "        self.valuehead = torch.nn.Sequential(*value)\n",
    "        #policy head\n",
    "        policy = []\n",
    "        policy.append(create_linear_torch(input_dims=input_space[0],output_dims=output_space,relu=False))\n",
    "        self.policyhead = torch.nn.Sequential(*policy)        \n",
    "        if \"softmax\" in configs:\n",
    "            self.policyhead = torch.nn.Sequential(*[self.policyhead,torch.nn.Softmax(dim=-1)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        value = self.valuehead(x)\n",
    "        policy = self.policyhead(x)\n",
    "        return value, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DualPolicyValueHeadTorch([4],2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
