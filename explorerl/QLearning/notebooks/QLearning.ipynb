{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explorerl.utils import *\n",
    "from explorerl.QLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear QLearning on MountainCar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "e = Env_Wrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Featurizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, featurizer = create_scaler_featurizer(env,make_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTF2(BaseAgent):\n",
    "    def __init__(self,epsilon=1.0, decay= 0.98, gamma=1.0, \n",
    "                 learning_rate=0.01, featurizer=None,scaler=None,use_bias = False):\n",
    "        super(QLearningTF2, self).__init__(gamma, \n",
    "                 learning_rate, featurizer,scaler,use_bias)\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.name = \"QLearningTF\"\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.original_configs = {\"epsilon\":self.epsilon,\"decay\":self.decay}\n",
    "        \n",
    "    def initialize_model(self,observation_space,action_space):\n",
    "        self.epsilon = self.original_configs[\"epsilon\"]\n",
    "        self.decay = self.original_configs[\"decay\"]\n",
    "        self.observation_space = observation_space[0]\n",
    "        self.action_space = action_space\n",
    "        input_space = self.observation_space  \n",
    "        if self.featurizer:\n",
    "            input_space = self.featurizer.transform([np.ones(self.observation_space)]).flatten().shape[0]\n",
    "        if self.use_bias:\n",
    "            input_space += 1\n",
    "        \n",
    "                \n",
    "#         for action in range(self.action_space):\n",
    "        estim = LinearEstimatorTF(input_space=input_space,output_space=self.action_space)\n",
    "        self.model[\"outputs\"] = estim\n",
    "        \n",
    "                \n",
    "        def mse_loss(model,predictions,targets):\n",
    "            return tf.losses.mean_squared_error(targets,predictions) + tf.add_n(model.losses)\n",
    "#             return tf.reduce_mean(tf.square(tf.subtract(predictions,targets))) + tf.add_n(model.losses)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        def train_step(model,inputs,targets):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(inputs)\n",
    "                total_loss = mse_loss(model,predictions,targets)\n",
    "            gradients = tape.gradient(total_loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "                \n",
    "        self.model[\"loss\"] = mse_loss\n",
    "        self.model[\"training_op\"] = train_step\n",
    "        print(\"Model Created!\")\n",
    "    \n",
    "    def update_hyper_params(self,episode):\n",
    "        self.epsilon *= (self.decay**episode)\n",
    "        \n",
    "    def train_policy(self):\n",
    "        return self.epsilon_greedy()\n",
    "    \n",
    "    def test_policy(self):\n",
    "        return self.greedy()\n",
    "    \n",
    "    def epsilon_greedy(self):\n",
    "        def act(obs):\n",
    "#             for action in range(self.action_space):\n",
    "            estimator = self.model[\"outputs\"]\n",
    "            qvals = estimator(obs)\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return np.random.choice(self.action_space) , qvals\n",
    "            return np.argmax(qvals[0]) , qvals\n",
    "        return act\n",
    "                \n",
    "    def greedy(self):\n",
    "        def act(obs):\n",
    "            qvals = []\n",
    "#             for action in range(self.action_space):\n",
    "            estimator = self.model[\"outputs\"]\n",
    "            qvals = estimator(obs)\n",
    "            return np.argmax(qvals[0]) , qvals\n",
    "        return act\n",
    "    \n",
    "    def train_iter(self,policy,action,values,obs,next_obs,reward,done):\n",
    "        training_op = self.model[\"training_op\"]\n",
    "        acc, qvals = policy(obs)\n",
    "        next_action , next_qs = policy(next_obs)\n",
    "        target = np.array(qvals)\n",
    "        target[0][action] = reward\n",
    "        if done == False:\n",
    "            target[0][action] = reward + self.gamma*np.max(next_qs)\n",
    "        target = tf.stop_gradient(target)\n",
    "        training_op(self.model[\"outputs\"],obs,target)\n",
    "        \n",
    "    def train(self,env,episodes=200,early_stop=False,stop_criteria=20):\n",
    "        prev_avg = -float('inf')\n",
    "        orig_epsilon = self.epsilon\n",
    "        bar = tqdm(np.arange(episodes),file=sys.stdout)\n",
    "        policy = self.epsilon_greedy()\n",
    "        criteria = 0 #stopping condition\n",
    "        loss = self.model[\"loss\"]\n",
    "        training_op = self.model[\"training_op\"]\n",
    "        for i in bar:\n",
    "            observation = env.reset()\n",
    "            self.epsilon *= (self.decay**i)\n",
    "            rewards = 0\n",
    "            end = 0\n",
    "            for t in range(10000):\n",
    "                action , qvals = policy(observation)\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "                rewards += reward\n",
    "                next_action , next_qs = policy(next_obs)\n",
    "                target = reward + self.gamma*np.max(next_qs)\n",
    "                inp = self.featurize_state(observation)\n",
    "                training_op(self.model[\"outputs\"][action],inp,target)\n",
    "                end = t\n",
    "                if done:\n",
    "                    break\n",
    "                observation = next_obs\n",
    "                \n",
    "            self.stats[\"num_steps\"].append(end)\n",
    "            self.stats[\"episodes\"].append(i)\n",
    "            self.stats[\"rewards\"].append(rewards)\n",
    "            avg = np.mean(self.stats[\"rewards\"][::-1][:25])\n",
    "            bar.set_description(\"Epsilon and reward {} : {}\".format(self.epsilon,avg))\n",
    "            \n",
    "            if avg < prev_avg:\n",
    "                criteria += 1\n",
    "                \n",
    "            if early_stop:\n",
    "                if criteria >= stop_criteria:\n",
    "                    break\n",
    "                    \n",
    "            prev_avg = avg\n",
    "        return self.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtf = QLearningTF2(learning_rate=0.001,use_bias=True,featurizer=featurizer,scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qtf.initialize_model(env.observation_space.shape,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon : 1.7862519804365075e-11, Num Steps : 179, Avg Reward with Window Size 25 : -186.88:  20%|██        | 50/250 [00:59<03:58,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "stat = e.train(qtf,episodes=250,plot=True)\n",
    "e.test(qtf,gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch = QLearningTorch(learning_rate=0.01,\n",
    "                   use_bias=True,featurizer=None,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = e.train(episodes=2500,agent=qtorch,plot=True)\n",
    "e.test(qtorch,gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Featurizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtf = QLearningTF(use_bias=True,featurizer=None,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = e.train(qtf,plot=True)\n",
    "e.test(qtf,gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtorch = QLearningTorch(learning_rate=0.01,\n",
    "                   use_bias=True,featurizer=None,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = e.train(qtorch,plot=True)\n",
    "e.test(qtorch,gif=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
